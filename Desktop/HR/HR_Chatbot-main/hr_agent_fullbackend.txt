<summary>

<header>
RepopackPy Output File
This file was generated by RepopackPy on: 2025-01-04T11:11:37.116221
</header>

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository structure
3. Repository files, each consisting of:
    - File path as an attribute
    - Full contents of the file
</file_format>

<usage_guidelines>
1. This file should be treated as read-only. Any changes should be made to the
    original repository files, not this packed version.
2. When processing this file, use the file path attributes to distinguish
    between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
    the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation.
</notes>

<additional_info>
For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py
</additional_info>

</summary>

<repository_structure>
agent.py
app.py
config.py
hardcode.py
rag_system.py
requirments.txt
tools\__init__.py
tools\rag_tool.py
tools\ticket_tool.py
tools\vacation_tool.py
</repository_structure>

<repository_files>

<file path="agent.py">
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import initialize_agent, Tool
from langchain.tools import StructuredTool
from langchain.agents import AgentType , StructuredChatAgent
from langchain.schema import HumanMessage
import pandas as pd
from datetime import datetime
import os
from typing import Dict, List, Optional
from tools.rag_tool import RAGTool
from tools.vacation_tool import VacationTool
from tools.ticket_tool import TicketTool
import json 
from langchain.tools import StructuredTool
from pydantic import BaseModel, Field

class VacationRequestSchema(BaseModel):
    employee_id: str = Field(description="The ID of the employee requesting vacation")
    start_date: str = Field(description="The start date of the vacation (YYYY-MM-DD)")
    end_date: str = Field(description="The end date of the vacation (YYYY-MM-DD)")
    request_type: str = Field(description="The type of request, e.g., 'vacation'")
class HRAgent:
    def __init__(self, google_api_key: str, vacations_file: str, tickets_file: str):
        """Initialize the HR Agent with necessary tools and configurations."""
        self.api_key = google_api_key
        
        # Initialize Tools
        self.rag_tool = RAGTool(google_api_key)
        self.vacation_tool = VacationTool(vacations_file)
        self.ticket_tool = TicketTool(tickets_file)

        # Initialize Gemini LLM
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=google_api_key,
            temperature=0.3,
            convert_system_message_to_human=True
        )

        # Define tools for the agent
        self.tools = [
            StructuredTool.from_function(
                name="PolicyQuery",
                func=self.rag_tool.query,
                description="Useful for answering questions about HR policies and regulations. Input should be the policy-related question as a string."
                ),
            StructuredTool.from_function(
                name="CheckVacationBalance",
                func=self.vacation_tool.check_balance,
                description="Check an employee's vacation balance. Input should be the employee_id as a string."
            ),
            StructuredTool.from_function(
                name="CreateVacationRequest",
                func=self.ticket_tool.create_ticket,
                description="Create a vacation request ticket. Input should be a dictionary with keys: 'employee_id', 'start_date' (YYYY-MM-DD), 'end_date' (YYYY-MM-DD), and 'request_type'."
            )
        ]

        # Initialize the agent
        self.agent = initialize_agent(
            self.tools,
            self.llm,
            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=3
        )

        # Define system prompt for the agent
        self.system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ:
1. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ† Ø­ÙˆÙ„ Ø³ÙŠØ§Ø³Ø§Øª ÙˆØ£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©
2. Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ† ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±ØµÙŠØ¯ Ø¥Ø¬Ø§Ø²Ø§ØªÙ‡Ù…
3. Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ† ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø²Ø©

Ø¹Ù†Ø¯ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:
- ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…Ù‡Ù†ÙŠØ§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·
- Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ø§Ù‹ØŒ Ø§Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­Ø§Ù‹
- Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§ØªØŒ ØªØ­Ù‚Ù‚ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ù† Ø§Ù„Ø±ØµÙŠØ¯ Ù‚Ø¨Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø·Ù„Ø¨"""

    def process_query(self, message: str, employee_id: Optional[str] = None) -> str:
        """Process a user query and return the response."""
        try:
            # Add context to the message if employee_id is provided
            if employee_id:
                context_message = f"Employee ID: {employee_id}\nQuery: {message}"
            else:
                context_message = message

            # Get agent response
            result = self.agent.invoke({
                "input": context_message,
                "chat_history": [],  # Could be extended to support chat history
                "system_prompt": self.system_prompt
            })

            return result.get('output', "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

        except Exception as e:
            print(f"Error processing query: {str(e)}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def add_document(self, filepath: str) -> bool:
        """Add a new document to the RAG system."""
        try:
            return self.rag_tool.add_document(filepath)
        except Exception as e:
            print(f"Error adding document: {str(e)}")
            return False

    def update_active_documents(self, document_list: List[str]) -> bool:
        """Update the list of active documents in the RAG system."""
        try:
            return self.rag_tool.update_active_documents(document_list)
        except Exception as e:
            print(f"Error updating active documents: {str(e)}")
            return False

    def get_vacation_balance(self, employee_id: str) -> Dict:
        """Get vacation balance for an employee."""
        try:
            return self.vacation_tool.check_balance(employee_id)
        except Exception as e:
            print(f"Error getting vacation balance: {str(e)}")
            return {"error": "Could not retrieve vacation balance"}

    def create_vacation_ticket(self, employee_id: str, start_date: str, end_date: str, request_type: str, notes: str = "") -> Dict:
        """Create a new vacation request ticket."""
        return self.ticket_tool.create_ticket(employee_id, start_date, end_date, request_type, notes)

def main():
    """Test the HR Agent"""
    try:
        # Initialize agent
        agent = HRAgent(
            google_api_key=os.getenv("GOOGLE_API_KEY"),
            vacations_file="data/vacations.csv",
            tickets_file="data/tickets.csv"
        )

        # Test queries
        test_queries = [
            "Ù…Ø§ Ù‡ÙŠ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ø³Ù†ÙˆÙŠØ©ØŸ",
            "ÙƒÙ… Ø±ØµÙŠØ¯ Ø¥Ø¬Ø§Ø²Ø§ØªÙŠ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØŸ",
            "Ø£Ø±ÙŠØ¯ ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨ Ø¥Ø¬Ø§Ø²Ø© Ù„Ù…Ø¯Ø© 5 Ø£ÙŠØ§Ù…"
        ]

        for query in test_queries:
            print(f"\nQuery: {query}")
            response = agent.process_query(query, employee_id="1001")
            print(f"Response: {response}")

    except Exception as e:
        print(f"Error in main: {str(e)}")

if __name__ == "__main__":
    main()
</file>

<file path="app.py">
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os
from datetime import datetime
from agent import HRAgent
import pandas as pd
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

app = Flask(__name__)
CORS(app)

# Configuration
UPLOAD_FOLDER = 'data/documents'
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'doc', 'txt'}
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Ensure upload directory exists
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Initialize HR Agent
agent = HRAgent(
    google_api_key=os.getenv('GOOGLE_API_KEY'),
    vacations_file='data/vacations.csv',
    tickets_file='data/tickets.csv'
)

def allowed_file(filename):
    """Check if file extension is allowed"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/chat', methods=['POST'])
def chat():
    """Handle chat messages"""
    try:
        data = request.json
        message = data.get('message', '').strip()
        employee_id = data.get('employee_id')
        
        if not message:
            return jsonify({'error': 'Message is required'}), 400
        
        # Process message through agent
        response = agent.process_query(message, employee_id)
        
        return jsonify({
            'response': response,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"Error in chat endpoint: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/admin/upload', methods=['POST'])
def upload_document():
    """Handle document uploads"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
            
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
            
        if not allowed_file(file.filename):
            return jsonify({'error': 'File type not allowed'}), 400
            
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        
        # Save the file
        file.save(filepath)
        
        # Process the document with RAG system
        agent.add_document(filepath)
        
        return jsonify({
            'message': 'Document uploaded successfully',
            'filename': filename
        })
        
    except Exception as e:
        print(f"Error in upload endpoint: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/admin/documents', methods=['GET'])
def list_documents():
    """List all available documents"""
    try:
        files = []
        for filename in os.listdir(app.config['UPLOAD_FOLDER']):
            if allowed_file(filename):
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                files.append({
                    'name': filename,
                    'size': os.path.getsize(filepath),
                    'uploaded': datetime.fromtimestamp(
                        os.path.getctime(filepath)
                    ).isoformat()
                })
                
        return jsonify({'documents': files})
        
    except Exception as e:
        print(f"Error listing documents: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/admin/documents', methods=['POST'])
def update_active_documents():
    """Update which documents are active in the RAG system"""
    try:
        data = request.json
        document_list = data.get('documents', [])
        
        # Update active documents in the agent
        agent.update_active_documents(document_list)
        
        return jsonify({
            'message': 'Active documents updated successfully',
            'active_documents': document_list
        })
        
    except Exception as e:
        print(f"Error updating active documents: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/employee/vacation-balance/<employee_id>', methods=['GET'])
def get_vacation_balance(employee_id):
    """Get vacation balance for an employee"""
    try:
        balance = agent.get_vacation_balance(employee_id)
        return jsonify(balance)
        
    except Exception as e:
        print(f"Error getting vacation balance: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/api/employee/vacation-request', methods=['POST'])
def submit_vacation_request():
    """Submit a new vacation request"""
    try:
        data = request.json
        employee_id = data.get('employee_id')
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        request_type = data.get('request_type')
        notes = data.get('notes', '')

        if not all([employee_id, start_date, end_date, request_type]):
            return jsonify({'error': 'Missing required fields'}), 400

        ticket = agent.create_vacation_ticket(
            employee_id=employee_id,
            start_date=start_date,
            end_date=end_date,
            request_type=request_type,
            notes=notes
        )
        return jsonify(ticket)

    except Exception as e:
        print(f"Error submitting vacation request: {str(e)}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

if __name__ == '__main__':
    # Load initial documents if any exist
    try:
        for filename in os.listdir(app.config['UPLOAD_FOLDER']):
            if allowed_file(filename):
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                agent.add_document(filepath)
        print("Initial documents loaded")
    except Exception as e:
        print(f"Error loading initial documents: {str(e)}")
    
    app.run(debug=True, port=5000)
</file>

<file path="config.py">
# config.py

import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Base Paths
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"

# Document paths
DOCUMENTS_DIR = DATA_DIR / "documents"
CHROMA_DIR = DATA_DIR / "chroma_db"

# CSV file paths
VACATIONS_FILE = DATA_DIR / "vacations.csv"
TICKETS_FILE = DATA_DIR / "tickets.csv"

# Ensure directories exist
DOCUMENTS_DIR.mkdir(parents=True, exist_ok=True)
CHROMA_DIR.mkdir(parents=True, exist_ok=True)

# RAG Configuration
RAG_CONFIG = {
    "chunk_size": 500,
    "chunk_overlap": 50,
    "model_name": "gemini-pro",
    "temperature": 0.3
}

# Default admin credentials (for demo purposes)
DEFAULT_ADMIN = {
    "username": "admin",
    "password": "admin123"  # In production, use proper authentication
}

# File upload settings
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'doc', 'txt'}
MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB max file size
</file>

<file path="hardcode.py">
from flask import Flask, request, jsonify
from flask_cors import CORS
import time

app = Flask(__name__)
CORS(app)

# Store conversation state
conversation_states = {}

# Predefined responses
RESPONSES = {
    "remote_work_policy": """ğŸ  Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯ ÙÙŠ Ø´Ø±ÙƒØªÙ†Ø§:

â€¢ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯ Ù„Ù…Ø¯Ø© 14 ÙŠÙˆÙ… ÙÙŠ Ø§Ù„Ø³Ù†Ø©
â€¢ ÙŠØ¬Ø¨ Ø¥Ø®Ø·Ø§Ø± Ù…Ø¯ÙŠØ±Ùƒ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù‚Ø¨Ù„ 3 Ø£ÙŠØ§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
â€¢ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø­Ø¶ÙˆØ± Ù„Ù„Ù…ÙƒØªØ¨ ÙÙŠ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
â€¢ ÙŠØ¬Ø¨ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©
â€¢ ØªÙˆÙÙŠØ± ØªÙ‚Ø±ÙŠØ± Ø£Ø³Ø¨ÙˆØ¹ÙŠ Ø¹Ù† Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù†Ø¬Ø²Ø©

Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø£Ø®Ø±Ù‰ØŸ ğŸ’¼""",
    
    "paid_leave_question": """âœ… Ù†Ø¹Ù…ØŒ ÙØªØ±Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯ Ù‡ÙŠ Ø¥Ø¬Ø§Ø²Ø© Ù…Ø¯ÙÙˆØ¹Ø© Ù„Ù…Ø¯Ø© 14 ÙŠÙˆÙ….

Ù‡Ù„ ØªØ±ÙŠØ¯ ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨ Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¯Ø©ØŸ ğŸ“""",
    
    "leave_confirmation": """ğŸ‰ ØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯ Ø¨Ù†Ø¬Ø§Ø­!

â€¢ Ø±Ù‚Ù… Ø§Ù„Ø·Ù„Ø¨: #WFH-2024-001
â€¢ Ø§Ù„Ù…Ø¯Ø©: 14 ÙŠÙˆÙ…
â€¢ Ø§Ù„Ø­Ø§Ù„Ø©: Ø¨Ø§Ù†ØªØ¸Ø§Ø± Ù…ÙˆØ§ÙÙ‚Ø© Ø§Ù„Ù…Ø¯ÙŠØ±

Ø³ÙŠØªÙ… Ø¥Ø´Ø¹Ø§Ø±Ùƒ Ø¹Ø¨Ø± Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙˆØ± Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ù„Ø¨Ùƒ ğŸ“§"""
}

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    message = data.get('message', '').strip()
    conversation_id = data.get('conversation_id', 'default')
    
    # Simulate processing time
    time.sleep(1)
    
    # Get current state
    current_state = conversation_states.get(conversation_id, 'initial')
    
    # Process message based on current state and content
    if "Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨Ø¹Ø¯" in message:
        conversation_states[conversation_id] = 'asked_policy'
        return jsonify({
            "response": RESPONSES["remote_work_policy"],
            "conversation_id": conversation_id
        })
    
    elif "Ø§Ø¬Ø§Ø²Ø© Ù…Ø¯ÙÙˆØ¹Ø©" in message and current_state == 'asked_policy':
        conversation_states[conversation_id] = 'asked_paid'
        return jsonify({
            "response": RESPONSES["paid_leave_question"],
            "conversation_id": conversation_id
        })
    
    elif any(word in message.lower() for word in ["Ù†Ø¹Ù…", "Ø§Ø¬Ù„", "Ù…ÙˆØ§ÙÙ‚"]) and current_state == 'asked_paid':
        conversation_states[conversation_id] = 'confirmed'
        return jsonify({
            "response": RESPONSES["leave_confirmation"],
            "conversation_id": conversation_id
        })
    
    # Default response
    return jsonify({
        "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ØŸ ğŸ¤”",
        "conversation_id": conversation_id
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)
</file>

<file path="rag_system.py">
# rag_system.py

import os
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """Metadata for processed documents"""
    title: str
    file_type: str
    created_date: str
    collection_id: str
    page_count: int
    status: str  # 'processing', 'active', 'error'
    error_message: Optional[str] = None
    last_updated: str = datetime.now().isoformat()

@dataclass
class ProcessedDocument:
    """Processed document information"""
    chunks: List[str]
    metadata: DocumentMetadata
    collection_path: str
    total_chunks: int
    embedding_status: str  # 'pending', 'completed', 'error'

class DocumentProcessor:
    """Handles document processing and embedding"""
    
    def __init__(self, api_key: str, base_path: str = "./data"):
        """
        Initialize the document processor.
        
        Args:
            api_key: Google API key for Gemini
            base_path: Base path for storing documents and collections
        """
        self.api_key = os.getenv("GOOGLE_API_KEY", api_key)
        self.base_path = base_path
        genai.configure(api_key=api_key)
        
        # Initialize embeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=api_key
        )
        
        # Configure text splitter for Arabic and English
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", "ØŸ", "ØŒ", " "]
        )
        
        # Ensure directory structure exists
        self.docs_path = Path(base_path) / "documents"
        self.chroma_path = Path(base_path) / "chroma_db"
        self.docs_path.mkdir(parents=True, exist_ok=True)
        self.chroma_path.mkdir(parents=True, exist_ok=True)

    def process_document(self, file_path: str) -> ProcessedDocument:
        """
        Process a document and prepare it for embedding.
        
        Args:
            file_path: Path to the document file
            
        Returns:
            ProcessedDocument object containing chunks and metadata
        """
        try:
            logger.info(f"Starting document processing: {file_path}")
            
            # Extract text based on file type
            text = self._extract_text(file_path)
            
            # Create document metadata
            metadata = self._create_metadata(file_path)
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text)
            
            # Create collection path
            collection_path = self.chroma_path / metadata.collection_id
            
            # Create processed document
            processed_doc = ProcessedDocument(
                chunks=chunks,
                metadata=metadata,
                collection_path=str(collection_path),
                total_chunks=len(chunks),
                embedding_status='pending'
            )
            
            logger.info(f"Document processed successfully: {metadata.collection_id}")
            return processed_doc
            
        except Exception as e:
            error_msg = f"Error processing document: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def _extract_text(self, file_path: str) -> str:
        """Extract text from different document types."""
        file_type = Path(file_path).suffix.lower()
        
        try:
            if file_type == '.pdf':
                return self._extract_pdf_text(file_path)
            elif file_type in ['.docx', '.doc']:
                return self._extract_word_text(file_path)
            elif file_type == '.txt':
                return self._extract_txt_text(file_path)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
                
        except Exception as e:
            raise Exception(f"Error extracting text: {str(e)}")

    def _extract_pdf_text(self, file_path: str) -> str:
        """Extract text from PDF files."""
        try:
            import pypdf
            
            with open(file_path, 'rb') as file:
                pdf = pypdf.PdfReader(file)
                text = ""
                for page in pdf.pages:
                    text += page.extract_text() + "\n\n"
                return text
                
        except Exception as e:
            raise Exception(f"Error extracting PDF text: {str(e)}")

    def _extract_word_text(self, file_path: str) -> str:
        """Extract text from Word documents."""
        try:
            from docx import Document
            
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
            
        except Exception as e:
            raise Exception(f"Error extracting Word text: {str(e)}")

    def _extract_txt_text(self, file_path: str) -> str:
        """Extract text from txt files."""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except Exception as e:
            raise Exception(f"Error extracting text file text: {str(e)}")

    def _create_metadata(self, file_path: str) -> DocumentMetadata:
        """Create metadata for a document."""
        file_path = Path(file_path)
        creation_time = datetime.fromtimestamp(file_path.stat().st_ctime)
        
        return DocumentMetadata(
            title=file_path.name,
            file_type=file_path.suffix[1:],  # Remove the dot
            created_date=creation_time.isoformat(),
            collection_id=f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            page_count=0,  # Will be updated based on document type
            status='processing'
        )

    def create_embeddings(self, processed_doc: ProcessedDocument) -> Chroma:
        """Create embeddings for processed document chunks."""
        try:
            logger.info(f"Creating embeddings for document: {processed_doc.metadata.collection_id}")
            
            # Prepare metadata for each chunk
            chunk_metadata = []
            for i, chunk in enumerate(processed_doc.chunks):
                chunk_metadata.append({
                    "chunk_id": str(i),
                    "document_title": processed_doc.metadata.title,
                    "collection_id": processed_doc.metadata.collection_id
                })
            
            # Create and persist vector store
            vectorstore = Chroma.from_texts(
                texts=processed_doc.chunks,
                metadatas=chunk_metadata,
                embedding=self.embeddings,
                persist_directory=processed_doc.collection_path
            )
            
            logger.info(f"Embeddings created successfully: {processed_doc.metadata.collection_id}")
            return vectorstore
            
        except Exception as e:
            error_msg = f"Error creating embeddings: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
        
        
# Continuing rag_system.py

class HRRAGSystem:
    """Main RAG system for HR document queries"""
    
    def __init__(self, api_key: str, base_path: str = "./data"):
        """
        Initialize the HR RAG system.
        
        Args:
            api_key: Google API key for Gemini
            base_path: Base path for data storage
        """
        self.api_key = api_key
        self.base_path = Path(base_path)
        self.doc_processor = DocumentProcessor(api_key, base_path)
        
        # Initialize LLM
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=api_key,
            temperature=0.3,
            convert_system_message_to_human=True
        )
        
        # Keep track of active collections
        self.active_collections: Dict[str, Chroma] = {}
        
        # Initialize from existing collections if any
        self._load_existing_collections()

    def _load_existing_collections(self):
        """Load existing collections from chroma_db directory."""
        try:
            chroma_path = self.base_path / "chroma_db"
            if chroma_path.exists():
                for collection_dir in chroma_path.iterdir():
                    if collection_dir.is_dir():
                        try:
                            vectorstore = Chroma(
                                persist_directory=str(collection_dir),
                                embedding_function=self.doc_processor.embeddings
                            )
                            self.active_collections[collection_dir.name] = vectorstore
                            logger.info(f"Loaded existing collection: {collection_dir.name}")
                        except Exception as e:
                            logger.error(f"Error loading collection {collection_dir.name}: {str(e)}")
        
        except Exception as e:
            logger.error(f"Error loading existing collections: {str(e)}")

    def process_document(self, file_path: str) -> str:
        """
        Process a new document and add it to the RAG system.
        
        Args:
            file_path: Path to the document file
            
        Returns:
            collection_id: ID of the created collection
        """
        try:
            # Process document
            processed_doc = self.doc_processor.process_document(file_path)
            
            # Create embeddings
            vectorstore = self.doc_processor.create_embeddings(processed_doc)
            
            # Add to active collections
            self.active_collections[processed_doc.metadata.collection_id] = vectorstore
            
            # Update metadata status
            processed_doc.metadata.status = 'active'
            processed_doc.embedding_status = 'completed'
            
            return processed_doc.metadata.collection_id
            
        except Exception as e:
            logger.error(f"Error processing document: {str(e)}")
            raise

    def query(self, question: str, collection_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Query the RAG system with a question.
        
        Args:
            question: The question to ask
            collection_ids: Optional list of specific collection IDs to query
            
        Returns:
            Dict containing answer and source documents
        """
        try:
            # If no specific collections provided, use all active ones
            collections_to_query = []
            if collection_ids:
                for cid in collection_ids:
                    if cid in self.active_collections:
                        collections_to_query.append(self.active_collections[cid])
            else:
                collections_to_query = list(self.active_collections.values())

            if not collections_to_query:
                return {
                    "answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„Ø¨Ø­Ø«.",
                    "source_documents": []
                }

            # Combine results from all collections
            all_docs = []
            for vectorstore in collections_to_query:
                docs = vectorstore.similarity_search(question, k=2)
                all_docs.extend(docs)

            # Sort by relevance (assumed from order) and take top results
            context = "\n".join(doc.page_content for doc in all_docs[:3])

            # Generate response using LLM
            prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„.
            Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªÙˆÙØ±ØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨ØµØ±Ø§Ø­Ø©.

            Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

            Ø§Ù„Ø³ÙŠØ§Ù‚:
            {context}

            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

            response = self.llm.invoke(prompt)

            return {
                "answer": response.content,
                "source_documents": all_docs[:3]
            }

        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            return {
                "answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„.",
                "source_documents": []
            }

    def merge_collections(self, collection_ids: List[str]) -> Optional[str]:
        """
        Merge multiple collections into a new one.
        
        Args:
            collection_ids: List of collection IDs to merge
            
        Returns:
            New collection ID if successful, None otherwise
        """
        try:
            # Validate collections exist
            collections = []
            for cid in collection_ids:
                if cid in self.active_collections:
                    collections.append(self.active_collections[cid])
                else:
                    raise ValueError(f"Collection not found: {cid}")

            # Create new collection ID
            merged_id = f"merged_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            merged_path = self.base_path / "chroma_db" / merged_id

            # Get all documents from collections
            all_texts = []
            all_metadata = []
            
            for vectorstore in collections:
                docs = vectorstore.get()
                all_texts.extend(docs['documents'])
                all_metadata.extend(docs['metadatas'])

            # Create new merged collection
            merged_vectorstore = Chroma.from_texts(
                texts=all_texts,
                metadatas=all_metadata,
                embedding=self.doc_processor.embeddings,
                persist_directory=str(merged_path)
            )

            # Add to active collections
            self.active_collections[merged_id] = merged_vectorstore

            return merged_id

        except Exception as e:
            logger.error(f"Error merging collections: {str(e)}")
            return None

    def remove_collection(self, collection_id: str) -> bool:
        """
        Remove a collection from the system.
        
        Args:
            collection_id: ID of the collection to remove
            
        Returns:
            bool: True if successful, False otherwise
        """
        try:
            if collection_id in self.active_collections:
                # Remove from active collections
                del self.active_collections[collection_id]
                
                # Remove directory
                collection_path = self.base_path / "chroma_db" / collection_id
                if collection_path.exists():
                    import shutil
                    shutil.rmtree(str(collection_path))
                
                logger.info(f"Removed collection: {collection_id}")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"Error removing collection: {str(e)}")
            return False

    def get_active_collections(self) -> List[Dict[str, Any]]:
        """
        Get information about all active collections.
        
        Returns:
            List of collection information dictionaries
        """
        try:
            collections_info = []
            for cid, vectorstore in self.active_collections.items():
                try:
                    docs = vectorstore.get()
                    collections_info.append({
                        "collection_id": cid,
                        "document_count": len(docs['documents']),
                        "created": datetime.fromtimestamp(
                            (self.base_path / "chroma_db" / cid).stat().st_ctime
                        ).isoformat()
                    })
                except Exception as e:
                    logger.error(f"Error getting info for collection {cid}: {str(e)}")
                    
            return collections_info
            
        except Exception as e:
            logger.error(f"Error getting collections info: {str(e)}")
            return []
</file>

<file path="requirments.txt">
# Core dependencies
flask>=2.0.0
flask-cors>=4.0.0
python-dotenv>=0.19.0

# Document processing
python-docx>=0.8.11
pypdf>=3.0.0

# LangChain and AI
langchain>=0.1.0
langchain-google-genai>=0.0.5
google-generativeai>=0.3.0
chromadb>=0.4.14

# Vector stores and embeddings
sentence-transformers>=2.2.2

# Data handling
pandas>=2.0.0
numpy>=1.21.0

# Utilities
pathlib>=1.0.1
python-dateutil>=2.8.2
werkzeug>=2.0.0

# Optional: for proper Arabic text handling
arabic-reshaper>=3.0.0
python-bidi>=0.4.2
</file>

<file path="tools\rag_tool.py">
# tools/rag_tool.py
from typing import Dict, List, Optional
import os
from rag_system import HRRAGSystem

class RAGTool:
    """Tool for handling HR document queries using RAG system"""
    
    def __init__(self, google_api_key: str):
        """Initialize RAG Tool with configurations."""
        self.api_key = google_api_key
        self.rag_system = HRRAGSystem(google_api_key)
        self.active_docs = []

    def query(self, question: str) -> str:
        """Query the RAG system with a question."""
        try:
            response = self.rag_system.query(question)
            return response.get('answer', 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©.')
        except Exception as e:
            print(f"Error in RAG query: {str(e)}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„."

    def add_document(self, filepath: str) -> bool:
        """Add a new document to the RAG system."""
        try:
            collection_id = self.rag_system.process_document(filepath)
            if collection_id:
                self.active_docs.append(filepath)
                return True
            return False
        except Exception as e:
            print(f"Error adding document: {str(e)}")
            return False

    def update_active_documents(self, document_list: List[str]) -> bool:
        """Update which documents are active in the system."""
        try:
            # Process any new documents
            for doc in document_list:
                if doc not in self.active_docs:
                    self.add_document(doc)
            
            # Update active documents
            self.active_docs = document_list
            return True
        except Exception as e:
            print(f"Error updating active documents: {str(e)}")
            return False
</file>

<file path="tools\ticket_tool.py">
# tools/ticket_tool.py
import pandas as pd
from datetime import datetime
from typing import Dict
import json

class TicketTool:
    """Tool for managing vacation request tickets"""
    
    def __init__(self, tickets_file: str):
        """Initialize Ticket Tool with the CSV file path."""
        self.tickets_file = tickets_file
        self._ensure_file_exists()

    def _ensure_file_exists(self):
        """Create tickets file if it doesn't exist."""
        try:
            df = pd.read_csv(self.tickets_file)
        except FileNotFoundError:
            # Create new file with structure
            df = pd.DataFrame(columns=[
                'ticket_id', 'employee_id', 'request_type',
                'start_date', 'end_date', 'days_count',
                'status', 'manager_id', 'request_date',
                'response_date', 'notes'
            ])
            df.to_csv(self.tickets_file, index=False)

    def _generate_ticket_id(self) -> str:
        """Generate a unique ticket ID."""
        try:
            df = pd.read_csv(self.tickets_file)
            if df.empty:
                return f"VT{datetime.now().year}001"
            
            # Get last ticket number and increment
            last_ticket = df['ticket_id'].iloc[-1]
            ticket_num = int(last_ticket[-3:]) + 1
            return f"VT{datetime.now().year}{ticket_num:03d}"
            
        except Exception:
            # Fallback to timestamp-based ID
            return f"VT{datetime.now().strftime('%Y%m%d%H%M%S')}"

    def create_ticket(self, employee_id: str, start_date: str, end_date: str, request_type: str, notes: str = "") -> Dict:
        """Create a new vacation request ticket."""
        try:
            # No need for JSON parsing anymore
            days_count = (datetime.strptime(start_date,'%Y-%m-%d') - datetime.strptime(end_date, '%Y-%m-%d')).days + 1

            new_ticket = {
                'ticket_id': self._generate_ticket_id(),
                'employee_id': employee_id,
                'request_type': request_type,
                'start_date': start_date,
                'end_date': end_date,
                'days_count': days_count,
                'status': 'pending',
                'manager_id': None,
                'request_date': datetime.now().strftime('%Y-%m-%d'),
                'response_date': None,
                'notes': notes
            }
            
            # Add to CSV
            df = pd.read_csv(self.tickets_file)
            df = pd.concat([df, pd.DataFrame([new_ticket])], ignore_index=True)
            df.to_csv(self.tickets_file, index=False)
            
            return {
                "status": "success",
                "message": "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø¨Ù†Ø¬Ø§Ø­",
                "ticket_id": new_ticket['ticket_id']
            }
            
        except Exception as e:
            print(f"Error creating ticket: {str(e)}")
            return {
                "error": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø©",
                "status": "error"
            }

    def update_ticket_status(self, ticket_id: str, status: str, 
                           manager_id: str, notes: str = "") -> Dict:
        """Update the status of a ticket."""
        try:
            df = pd.read_csv(self.tickets_file)
            mask = df['ticket_id'] == ticket_id
            
            if not any(mask):
                return {
                    "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø·Ù„Ø¨",
                    "status": "not_found"
                }
            
            # Update ticket
            df.loc[mask, 'status'] = status
            df.loc[mask, 'manager_id'] = manager_id
            df.loc[mask, 'response_date'] = datetime.now().strftime('%Y-%m-%d')
            if notes:
                df.loc[mask, 'notes'] = notes
            
            # Save changes
            df.to_csv(self.tickets_file, index=False)
            
            return {
                "status": "success",
                "message": "ØªÙ… ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¨Ù†Ø¬Ø§Ø­"
            }
            
        except Exception as e:
            print(f"Error updating ticket: {str(e)}")
            return {
                "error": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨",
                "status": "error"
            }

    def get_employee_tickets(self, employee_id: str) -> Dict:
        """Get all tickets for an employee."""
        try:
            df = pd.read_csv(self.tickets_file)
            employee_tickets = df[df['employee_id'] == employee_id].to_dict('records')
            
            return {
                "status": "success",
                "tickets": employee_tickets
            }
            
        except Exception as e:
            print(f"Error getting tickets: {str(e)}")
            return {
                "error": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª",
                "status": "error"
            }
</file>

<file path="tools\vacation_tool.py">
# tools/vacation_tool.py
import pandas as pd
from typing import Dict
from datetime import datetime

class VacationTool:
    """Tool for checking and managing employee vacation balances"""
    
    def __init__(self, vacations_file: str):
        """Initialize Vacation Tool with the CSV file path."""
        self.vacations_file = vacations_file
        self._ensure_file_exists()

    def _ensure_file_exists(self):
        """Create vacations file if it doesn't exist."""
        try:
            df = pd.read_csv(self.vacations_file)
        except FileNotFoundError:
            # Create new file with structure
            df = pd.DataFrame(columns=[
                'employee_id', 'name', 'position', 'department',
                'annual_balance', 'used_days', 'remaining_balance',
                'last_updated'
            ])
            df.to_csv(self.vacations_file, index=False)

    def check_balance(self, employee_id: str) -> Dict:
        """Check vacation balance for an employee."""
        try:
            df = pd.read_csv(self.vacations_file)
            employee = df[df['employee_id'] == int(employee_id)]
            
            if employee.empty:
                return {
                    "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¸Ù",
                    "status": "not_found"
                }
            
            return {
                "status": "success",
                "employee_id": str(employee_id),
                "name": employee.iloc[0]['name'],
                "annual_balance": float(employee.iloc[0]['annual_balance']),
                "used_days": float(employee.iloc[0]['used_days']),
                "remaining_balance": float(employee.iloc[0]['remaining_balance']),
                "last_updated": employee.iloc[0]['last_updated']
            }
            
        except Exception as e:
            print(f"Error checking balance: {str(e)}")
            return {
                "error": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ØµÙŠØ¯",
                "status": "error"
            }

    def update_balance(self, employee_id: str, days_used: float) -> Dict:
        """Update vacation balance after request approval."""
        try:
            df = pd.read_csv(self.vacations_file)
            mask = df['employee_id'] == int(employee_id)
            
            if not any(mask):
                return {
                    "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¸Ù",
                    "status": "not_found"
                }
            
            # Update balance
            df.loc[mask, 'used_days'] += days_used
            df.loc[mask, 'remaining_balance'] -= days_used
            df.loc[mask, 'last_updated'] = datetime.now().strftime('%Y-%m-%d')
            
            # Save changes
            df.to_csv(self.vacations_file, index=False)
            
            return {
                "status": "success",
                "message": "ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±ØµÙŠØ¯ Ø¨Ù†Ø¬Ø§Ø­"
            }
            
        except Exception as e:
            print(f"Error updating balance: {str(e)}")
            return {
                "error": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±ØµÙŠØ¯",
                "status": "error"
            }
</file>

<file path="tools\__init__.py">
# tools/__init__.py

from .rag_tool import RAGTool
from .vacation_tool import VacationTool
from .ticket_tool import TicketTool

__all__ = [
    'RAGTool',
    'VacationTool',
    'TicketTool'
]
</file>

</repository_files>
